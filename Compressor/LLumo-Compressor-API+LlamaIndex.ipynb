{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1TbBvMKT4xJDUOJ72gSPgHYItCBv1Zl9m","timestamp":1721019144365}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction\n","\n","Welcome to this Colab Notebook tutorial! In this guide, we will demonstrate how to efficiently incorporate the LLUMO Compressor API into your LlamaIndex pipeline to save costs while utilizing Large Language Models (LLMs). Specifically, we will walk you through the process of extracting answers from a PDF document by integrating LlamaIndex for vector search, OpenAI for generating answers, and LLUMO to compress prompts before sending them to OpenAI.\n","\n","## What You'll Learn\n","\n","1. **LlamaIndex Integration**: How to use LlamaIndex for efficient vector search to locate relevant text passages within a PDF document.\n","2. **OpenAI Integration**: How to utilize OpenAI's LLM for generating answers based on the extracted text passages.\n","3. **LLUMO Compressor API**: How to integrate the LLUMO Compressor API into your pipeline to compress prompts, reducing the amount of data sent to OpenAI and thus saving on API usage costs.\n","\n","## Why This Tutorial?\n","\n","Using LLMs like OpenAI's GPT-4 can be expensive, especially when dealing with large documents or frequent queries. The LLUMO Compressor API helps mitigate these costs by compressing prompts before they are sent to the LLM, ensuring you get the same high-quality responses at a lower cost. This tutorial is particularly useful for developers and data scientists who want to optimize their LlamaIndex pipelines by incorporating cost-saving measures without compromising on performance.\n","\n","By the end of this tutorial, you will have a fully functional pipeline that efficiently extracts and processes information from PDFs, providing accurate answers while minimizing costs through prompt compression.\n","\n","Let's get started!"],"metadata":{"id":"UWJknX9YAKhu"}},{"cell_type":"markdown","source":["# Step 1: Installing Required Python Libraries\n","\n","In this first step, we will install several essential Python libraries that are required to build our LlamaINdex pipeline with the LLUMO Compressor API. Each library serves a specific purpose in our workflow, enabling us to handle natural language processing, PDF reading, environment variable management, similarity search, and interaction with OpenAI's API. Here's a detailed overview of each library we will be using:\n","\n","1. **llama-index**: Specifically, we will walk you through the process of extracting answers from a PDF document by integrating LlamaIndex for vector search, OpenAI for generating answers, and LLUMO to compress prompts before sending them to OpenAI.\n","\n","2. **PyPDF2**: PyPDF2 is a pure Python library that allows us to read and manipulate PDF files. In this tutorial, we will use PyPDF2 to extract text from PDF documents, which will then be processed and analyzed using our natural language processing tools.\n","\n","3. **python-dotenv**: This library is used for managing environment variables in Python. By using python-dotenv, we can securely store and access sensitive information such as API keys and other configuration settings needed for our project.\n","\n","\n","4. **openai**: The openai library allows us to interact with OpenAI's API, enabling us to use their powerful language models for generating answers. This library will be crucial for sending prompts and receiving responses from OpenAI's LLM.\n","\n","5. **requests and json**: These libraries are used for making HTTP requests and handling JSON data, respectively. We will use requests to communicate with external APIs, including the LLUMO Compressor API, and json to parse and manipulate JSON data returned by these APIs.\n","\n","Let's start by installing these libraries using the following pip command. This command will ensure that all the necessary libraries are installed and ready to use in our Colab environment."],"metadata":{"id":"dfHCb-hJykxN"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RoOX9X2bJQTo","executionInfo":{"status":"ok","timestamp":1725611310751,"user_tz":-330,"elapsed":19820,"user":{"displayName":"Jatin Agarwal","userId":"14406162079289589360"}},"outputId":"90d389af-6ddc-4081-e8c4-589ef5652c63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-index==0.9.48\n","  Downloading llama_index-0.9.48-py3-none-any.whl.metadata (8.4 kB)\n","Collecting PyPDF2==3.0.1\n","  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n","Collecting python-dotenv==1.0.1\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Collecting openai==1.12.0\n","  Downloading openai-1.12.0-py3-none-any.whl.metadata (18 kB)\n","Collecting pypdf\n","  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: SQLAlchemy>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.48) (2.0.32)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (3.10.5)\n","Collecting dataclasses-json (from llama-index==0.9.48)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting deprecated>=1.2.9.3 (from llama-index==0.9.48)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n","Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index==0.9.48)\n","  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (2024.6.1)\n","Collecting httpx (from llama-index==0.9.48)\n","  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n","Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (3.3)\n","Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (3.8.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (1.26.4)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (2.1.4)\n","Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (2.32.3)\n","Collecting tenacity<9.0.0,>=8.2.0 (from llama-index==0.9.48)\n","  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n","Collecting tiktoken>=0.3.3 (from llama-index==0.9.48)\n","  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index==0.9.48) (4.12.2)\n","Collecting typing-inspect>=0.8.0 (from llama-index==0.9.48)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.12.0) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai==1.12.0) (1.7.0)\n","Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai==1.12.0) (2.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai==1.12.0) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai==1.12.0) (4.66.5)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (2.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index==0.9.48) (4.0.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.12.0) (3.8)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai==1.12.0) (1.2.2)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index==0.9.48) (1.16.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index==0.9.48) (2024.8.30)\n","Collecting httpcore==1.* (from httpx->llama-index==0.9.48)\n","  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n","Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index==0.9.48)\n","  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.48) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.48) (1.4.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index==0.9.48) (2024.5.15)\n","Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai==1.12.0) (2.20.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index==0.9.48) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index==0.9.48) (2.0.7)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index==0.9.48) (3.0.3)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index==0.9.48)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index==0.9.48)\n","  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.48) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.48) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index==0.9.48) (2024.1)\n","Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index==0.9.48) (24.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index==0.9.48) (1.16.0)\n","Downloading llama_index-0.9.48-py3-none-any.whl (15.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading openai-1.12.0-py3-none-any.whl (226 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n","Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n","Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: dirtyjson, tenacity, python-dotenv, PyPDF2, pypdf, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-index\n","  Attempting uninstall: tenacity\n","    Found existing installation: tenacity 9.0.0\n","    Uninstalling tenacity-9.0.0:\n","      Successfully uninstalled tenacity-9.0.0\n","Successfully installed PyPDF2-3.0.1 dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 llama-index-0.9.48 marshmallow-3.22.0 mypy-extensions-1.0.0 openai-1.12.0 pypdf-4.3.1 python-dotenv-1.0.1 tenacity-8.5.0 tiktoken-0.7.0 typing-inspect-0.9.0\n"]}],"source":["!pip install llama-index==0.9.48 PyPDF2==3.0.1 python-dotenv==1.0.1 openai==1.12.0 pypdf"]},{"cell_type":"markdown","source":["# Step 2: Importing Required Libraries\n","\n","Before we dive into the specific functions and steps of our pipeline, it's important to import all the necessary libraries that we'll be using throughout this notebook.\n","\n","### Explanation:\n","\n","1. **`import os`**: This module provides a way to interact with the operating system, including setting and retrieving environment variables.\n","\n","2. **`from getpass import getpass`**: This module allows us to securely prompt the user for sensitive information, such as API keys, without echoing the input back to the screen.\n","\n","3. **`from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Document`**: These classes are core components of LlamaIndex. They are used for creating and querying vector indexes, reading documents, and configuring the indexing and querying process.\n","\n","4. **`from llama_index.llms import OpenAI`**: This class is used to interact with OpenAI's API for generating responses from their language models.\n","\n","\n","5. **`import requests`**: This library is used for making HTTP requests, which we will need to communicate with external APIs, including the LLUMO Compressor API.\n","\n","6. **`import json`**: This library is used for parsing and manipulating JSON data, which is often the format of data exchanged between APIs."],"metadata":{"id":"fqqMxicTHRK1"}},{"cell_type":"code","source":["import os\n","from getpass import getpass\n","from llama_index import VectorStoreIndex, SimpleDirectoryReader, ServiceContext, Document\n","from llama_index.llms import OpenAI\n","import requests\n","import json"],"metadata":{"id":"E3_qPL6gQj3C","executionInfo":{"status":"ok","timestamp":1725611321460,"user_tz":-330,"elapsed":10711,"user":{"displayName":"Jatin Agarwal","userId":"14406162079289589360"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["# Step 3: Setting Up API Keys\n","\n","To interact with OpenAI's API and the LLUMO Compressor API, we need to provide our unique API keys for authentication. These keys are sensitive pieces of information that should be handled securely. In this step, we will use Python's `getpass` module to safely input our API keys and then store them in environment variables for later use.\n","\n","### Explanation:\n","\n","1. **Importing Required Modules**:\n","   - `getpass`: This module provides a way to securely prompt the user for input without echoing the input back to the screen. This is particularly useful for handling sensitive information like API keys.\n","   - `os`: This module provides a way to interact with the operating system, including setting environment variables.\n","\n","2. **Prompting for OpenAI API Key**:\n","   - `openai_api_key = getpass(\"Enter your OpenAI API key: \")`: This line prompts the user to enter their OpenAI API key. The input is not displayed on the screen for security reasons.\n","\n","3. **Prompting for LLUMO API Key**:\n","   - `llumo_api_key = getpass(\"Enter your LLUMO API key: \")`: Similarly, this line prompts the user to enter their LLUMO API key securely.\n","\n","4. **Storing API Keys in Environment Variables**:\n","   - `os.environ['OPENAI_API_KEY'] = openai_api_key`: This line stores the OpenAI API key in an environment variable named `OPENAI_API_KEY`.\n","   - `os.environ['LLUMO_API_KEY'] = llumo_api_key`: This line stores the LLUMO API key in an environment variable named `LLUMO_API_KEY`.\n","\n","5. **Deleting the Variables**:\n","   - `del openai_api_key`: This line deletes the variable `openai_api_key` from memory to ensure that the API key is not accidentally exposed or misused later in the code.\n","   - `del llumo_api_key`: This line deletes the variable `llumo_api_key` for the same reason.\n","\n","By following these steps, we ensure that our API keys are securely handled and stored, reducing the risk of accidental exposure. This setup is crucial for maintaining the security and integrity of our project."],"metadata":{"id":"igFZD-PTy5I3"}},{"cell_type":"code","source":["# Prompting the user to securely input their OpenAI API key\n","openai_api_key = getpass(\"Enter your OpenAI API key: \")\n","\n","# Prompting the user to securely input their LLUMO API key\n","llumo_api_key = getpass(\"Enter your LLUMO API key: \")\n","\n","# Storing the OpenAI API key in an environment variable\n","os.environ['OPENAI_API_KEY'] = openai_api_key\n","\n","# Storing the LLUMO API key in an environment variable\n","os.environ['LLUMO_API_KEY'] = llumo_api_key\n","\n","# Deleting the variables that hold the API keys to prevent them from being exposed in the code\n","del openai_api_key\n","del llumo_api_key"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fmsZi_lZQrox","executionInfo":{"status":"ok","timestamp":1725611357766,"user_tz":-330,"elapsed":36313,"user":{"displayName":"Jatin Agarwal","userId":"14406162079289589360"}},"outputId":"5a29a6e5-e244-4409-aa68-1b86dceb8457"},"execution_count":3,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key: ··········\n","Enter your LLUMO API key: ··········\n"]}]},{"cell_type":"markdown","source":["# Step 4: Loading and Extracting Text from PDF\n","\n","In this step, we define a function to read a PDF file, extract its text content, and process it for indexing. This function, load_and_process_pdf, utilizes the SimpleDirectoryReader class from LlamaIndex to read the PDF, and then manually chunks the text for more efficient processing. This function is a key component of our pipeline, as it allows us to convert the PDF content into a format that can be efficiently indexed and queried using LlamaIndex\n","### Explanation:\n","\n","1. **Defining the load_and_process_pdf Function:**\n","\n","* def `load_and_process_pdf(file_path):`: This line defines a function named `load_and_process_pdf` that takes a single argument, `file_path`, which is the path to the PDF file we want to read and process.\n","\n","\n","2. **Loading the PDF**:\n","\n","* `raw_documents = SimpleDirectoryReader(input_files=[file_path]).load_data()`: This line uses LlamaIndex's `SimpleDirectoryReader` to load the PDF file. It returns a list of `Document` objects, each representing a page or section of the PDF.\n","\n","\n","3. **Creating Smaller Text Chunks**:\n","\n","* We iterate through each `Document` object in `raw_documents` and split its text content into smaller chunks. This is done to ensure that each chunk is of a manageable size for processing and indexing.\n","* `text_chunks = [doc.text[i:i+1000] for i in range(0, len(doc.text), 800)]`: This line creates chunks of 1000 characters with an overlap of 200 characters between chunks.\n","\n","\n","4. **Creating Document Objects from Chunks:**\n","\n","* `documents.extend([Document(text=chunk) for chunk in text_chunks])`: This line creates new `Document` objects for each text chunk and adds them to the `documents` list.\n","\n","\n","5. **Configuring the Service Context**:\n","\n","* `service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))`: This line sets up the service context for LlamaIndex, specifying the OpenAI model to use for text generation.\n","\n","\n","6. **Creating the Vector Store Index**:\n","\n","* `index = VectorStoreIndex.from_documents(documents, service_context=service_context)`: This line creates a `VectorStoreIndex` from the processed documents, using the specified service context.\n","\n","\n","7. **Returning the Index:**\n","\n","* `return index`: After processing and indexing the PDF content, this line returns the created index.\n","\n","This function effectively loads the PDF, breaks it down into manageable chunks, and creates an index that can be efficiently queried. By using LlamaIndex's built-in classes and methods, we ensure that the PDF content is properly prepared for our question-answering system."],"metadata":{"id":"1djQFDcty_7V"}},{"cell_type":"code","source":["def load_and_process_pdf(file_path):\n","    # Load the PDF\n","    raw_documents = SimpleDirectoryReader(input_files=[file_path]).load_data()\n","\n","    # Create documents with smaller chunk sizes\n","    documents = []\n","    for doc in raw_documents:\n","        text_chunks = [doc.text[i:i+1000] for i in range(0, len(doc.text), 800)]  # 1000 chunk size, 200 overlap\n","        documents.extend([Document(text=chunk) for chunk in text_chunks])\n","\n","    # Configure Service Context\n","    service_context = ServiceContext.from_defaults(llm=OpenAI(temperature=0, model_name=\"gpt-3.5-turbo\"))\n","\n","    # Create index\n","    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n","\n","    return index"],"metadata":{"id":"2NZ3RT2ASbxP","executionInfo":{"status":"ok","timestamp":1725611357767,"user_tz":-330,"elapsed":20,"user":{"displayName":"Jatin Agarwal","userId":"14406162079289589360"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["# Step 5: Compressing Text with LLUMO API\n","\n","In this step, we define a function to compress text using the LLUMO Compressor API. This function sends a request to the LLUMO API, receives the compressed text, and calculates the compression percentage. The function also handles any errors that might occur during the process. Let's understand each step one by one\n","\n","\n","#### Retrieving the LLUMO API Key\n","```python\n","    # Retrieve the Llumo API key from environment variables\n","    LLUMO_API_KEY = os.getenv('LLUMO_API_KEY')\n","```\n","- This line retrieves the LLUMO API key from the environment variables using `os.getenv()`. The API key is essential for authenticating the request to the LLUMO API.\n","\n","#### Setting the LLUMO API Endpoint\n","```python\n","    # Set the Llumo API endpoint for text compression\n","    LLUMO_ENDPOINT = \"https://app.llumo.ai/api/compress\"\n","```\n","- This line sets the endpoint URL for the LLUMO API that handles text compression.\n","\n","#### Preparing Headers for the API Request\n","```python\n","    # Prepare headers for the API request\n","    headers = {\n","        \"Content-Type\": \"application/json\",\n","        \"Authorization\": f\"Bearer {LLUMO_API_KEY}\"\n","    }\n","```\n","- This block prepares the headers required for the API request, including the content type (`application/json`) and the authorization token (`Bearer {LLUMO_API_KEY}`).\n","\n","#### Preparing the Payload for the API Request\n","```python\n","    # Prepare the payload for the API request\n","    payload = {\"prompt\": text}\n","    if topic:\n","        payload[\"topic\"] = topic  # Add topic to payload if provided\n","```\n","- This block prepares the payload (data to be sent in the request) with the text to be compressed. If a topic is provided, it adds the topic to the payload.\n","\n","#### Sending the POST Request to LLUMO API\n","```python\n","    try:\n","        # Send POST request to Llumo API\n","        response = requests.post(LLUMO_ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()  # Raise an exception for bad status codes\n","```\n","- This block sends a POST request to the LLUMO API with the prepared payload and headers. The `raise_for_status()` method raises an exception if the request fails.\n","\n","#### Parsing the JSON Response\n","```python\n","        # Parse the JSON response\n","        result = response.json()\n","        data = result['data']['data']\n","        data_content = json.loads(data)\n","```\n","- This block parses the JSON response from the API. It extracts the relevant data and converts it from a JSON string to a Python dictionary.\n","\n","#### Extracting Compressed Text and Token Counts\n","```python\n","        # Extract compressed text and token counts from the response\n","        compressed_text = data_content.get('compressedPrompt', text)  # Use original text if compression fails\n","        initial_tokens = data_content.get('initialTokens', 0)\n","        final_tokens = data_content.get('finalTokens', 0)\n","```\n","- This block extracts the compressed text and token counts (initial and final) from the response. If compression fails, it defaults to using the original text.\n","\n","#### Calculating Compression Percentage\n","```python\n","        # Calculate compression percentage if token counts are available\n","        if initial_tokens and final_tokens:\n","            compression_percentage = ((initial_tokens - final_tokens) / initial_tokens) * 100\n","        else:\n","            compression_percentage = 0\n","```\n","- This block calculates the compression percentage based on the initial and final token counts. If these counts are not available, the compression percentage is set to 0.\n","\n","#### Returning the Results\n","```python\n","        # Return compressed text, success status, and compression statistics\n","        return compressed_text, True, compression_percentage, initial_tokens, final_tokens\n","```\n","- This block returns the compressed text, a success status, and the compression statistics (compression percentage, initial tokens, and final tokens).\n","\n","#### Handling Exceptions\n","```python\n","    except Exception as e:\n","        # If an error occurs, print the error and return original text with failure status\n","        print(f\"Error compressing text: {str(e)}\")\n","        return text, False, 0, 0, 0\n","```\n","- This block handles any exceptions that occur during the process, printing an error message and returning the original text with a failure status.\n","\n","### Final Code Integration\n","\n","We will now combine everything to get the final code."],"metadata":{"id":"WDSOK6S4Nadw"}},{"cell_type":"code","source":["def compress_with_llumo(text, topic=None):\n","    LLUMO_API_KEY = os.getenv('LLUMO_API_KEY')\n","    LLUMO_ENDPOINT = \"https://app.llumo.ai/api/compress\"\n","    headers = {\n","        \"Content-Type\": \"application/json\",\n","        \"Authorization\": f\"Bearer {LLUMO_API_KEY}\"\n","    }\n","    payload = {\"prompt\": text}\n","    if topic:\n","        payload[\"topic\"] = topic\n","\n","    try:\n","        response = requests.post(LLUMO_ENDPOINT, json=payload, headers=headers)\n","        response.raise_for_status()\n","        result = response.json()\n","        data = result['data']['data']\n","        data_content = json.loads(data)\n","\n","        compressed_text = data_content.get('compressedPrompt', text)\n","        initial_tokens = data_content.get('initialTokens', 0)\n","        final_tokens = data_content.get('finalTokens', 0)\n","\n","        if initial_tokens and final_tokens:\n","            compression_percentage = ((initial_tokens - final_tokens) / initial_tokens) * 100\n","        else:\n","            compression_percentage = 0\n","\n","        return compressed_text, True, compression_percentage, initial_tokens, final_tokens\n","    except Exception as e:\n","        print(f\"Error compressing text: {str(e)}\")\n","        return text, False, 0, 0, 0"],"metadata":{"id":"JC1OONTBNbn0","executionInfo":{"status":"ok","timestamp":1725611357767,"user_tz":-330,"elapsed":18,"user":{"displayName":"Jatin Agarwal","userId":"14406162079289589360"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# Step 6: Main Function - Integrating All Steps\n","\n","The `main` function serves as the entry point for our PDF Query Assistant. It integrates all the steps discussed previously, from uploading and processing a PDF file to querying the content and using LLUMO compression to optimize costs. We will go in details of each part of the `main` function:\n","\n","#### Initialization and PDF Upload\n","```python\n","    print(\"PDF Query Assistant with Llumo Compression\")\n","\n","    # Upload PDF\n","    from google.colab import files\n","    uploaded = files.upload()\n","\n","    if uploaded:\n","        pdf_file = list(uploaded.keys())[0]\n","```\n","- **Printing the Title**: Displays the title of the assistant.\n","- **Uploading the PDF**: Uses Google Colab's `files.upload()` to upload a PDF file.\n","- **Retrieving the File Name**: Gets the name of the uploaded file.\n","\n","#### Processing the PDF\n","```python\n","        # Process the PDF\n","        print(\"Processing PDF...\")\n","        index = load_and_process_pdf(pdf_file)\n","```\n","Calls the `load_and_process_pdf` function to extract text from the PDF and convert into\n","chunks.\n","\n","#### Handling User Query\n","```python\n","        # User Query Input\n","        query = input(\"Ask any question related to the content of the PDF: \")\n","\n","        if query:\n","            ## Similarity Search\n","            retriever = index.as_retriever(similarity_top_k=3)\n","            retrieved_nodes = retriever.retrieve(query)\n","\n","            # Prepare context for compression\n","            context = \" \".join([node.node.text for node in retrieved_nodes])\n","```\n","- **User Query Input**: Prompts the user to input a query related to the PDF content.\n","- **Similarity Search**: This block checks if the user provided a query. If so, it creates a retriever object using the processed PDF content (index) and retrieves the top 3 most similar nodes to the query.\n","- **Preparing Context**:This block prepares the context for compression by concatenating the text of the retrieved nodes.\n","\n","\n","#### Compressing Context with LLUMO\n","```python\n","            # Compress context using Llumo\n","            compressed_text, success, compression_percentage, initial_tokens, final_tokens = compress_with_llumo(context, topic=query)\n","\n","            if success:\n","                print(f\"Llumo Compression achieved: {compression_percentage:.2f}%\")\n","                print(f\"Initial tokens: {initial_tokens}\")\n","                print(f\"Final tokens: {final_tokens}\")\n","```\n","- **Compressing Context**: Calls the `compress_with_llumo` function to compress the context using LLUMO API.\n","- **Handling Success**: If compression is successful, prints the compression percentage and token counts.\n","\n","#### Generating Response with Compressed Context\n","```python\n","                # Generate response using compressed text\n","                llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","                response = llm.complete(compressed_text + \"\\n\\nQuestion: \" + query + \"\\nAnswer:\")\n","                print(\"Answer:\")\n","                print(response.text)\n","```\n","- **LLM Initialization**: Initializes a language model (`ChatOpenAI`) with specified parameters.\n","- This block checks if the compression was successful. If so, it prints the compression statistics and generates a response using the compressed text and the query. The response is generated using an OpenAI model (GPT-3.5-turbo) and is printed.\n","\n","#### Handling Compression Failure\n","```python\n","            else:\n","                print(\"Failed to compress the text with Llumo. Using original context.\")\n","                # Fallback: Use original context if compression fails\n","                llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","                response = llm.complete(context + \"\\n\\nQuestion: \" + query + \"\\nAnswer:\")\n","                print(\"Answer:\")\n","                print(response.text)\n","```\n","- This block handles the case where the compression fails. It prints a message indicating the failure and falls back to using the original context to generate the response using the OpenAI model.\n","\n","### Final Code Integration\n","\n","We have now integrated all the steps to get the final code in the `main` function. This comprehensive function handles PDF uploading, text extraction, text chunking, embedding generation, similarity search, text compression with LLUMO, and generating a response to the user's query using the compressed or original text."],"metadata":{"id":"UtfLMTKPQRKt"}},{"cell_type":"code","source":["def main():\n","    print(\"PDF Query Assistant with Llumo Compression\")\n","\n","    # Upload PDF\n","    from google.colab import files\n","    uploaded = files.upload()\n","\n","    if uploaded:\n","        pdf_file = list(uploaded.keys())[0]\n","\n","        # Process the PDF\n","        print(\"Processing PDF...\")\n","        index = load_and_process_pdf(pdf_file)\n","\n","        # User Query Input\n","        query = input(\"Ask any question related to the content of the PDF: \")\n","\n","        if query:\n","            # Similarity Search\n","            retriever = index.as_retriever(similarity_top_k=3)\n","            retrieved_nodes = retriever.retrieve(query)\n","\n","            # Prepare context for compression\n","            context = \" \".join([node.node.text for node in retrieved_nodes])\n","\n","            # Compress context using Llumo\n","            compressed_text, success, compression_percentage, initial_tokens, final_tokens = compress_with_llumo(context, topic=query)\n","\n","            if success:\n","                print(f\"Llumo Compression achieved: {compression_percentage:.2f}%\")\n","                print(f\"Initial tokens: {initial_tokens}\")\n","                print(f\"Final tokens: {final_tokens}\")\n","\n","                # Generate response using compressed text\n","                llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","                response = llm.complete(compressed_text + \"\\n\\nQuestion: \" + query + \"\\nAnswer:\")\n","                print(\"Answer:\")\n","                print(response.text)\n","            else:\n","                print(\"Failed to compress the text with Llumo. Using original context.\")\n","                # Fallback: Use original context if compression fails\n","                llm = OpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","                response = llm.complete(context + \"\\n\\nQuestion: \" + query + \"\\nAnswer:\")\n","                print(\"Answer:\")\n","                print(response.text)"],"metadata":{"id":"3nvkWPhtQPER","executionInfo":{"status":"ok","timestamp":1725611357768,"user_tz":-330,"elapsed":18,"user":{"displayName":"Jatin Agarwal","userId":"14406162079289589360"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# Step 7 : Running the main function\n","\n","By including this check, we ensure that the main() function is only executed when the script is run directly, providing flexibility and preventing unintended behavior when the script is imported elsewhere.\n","\n","It will ask you to upload a PDF and then when uploaded, it will receive input for the question you want to be answered."],"metadata":{"id":"5oe6Zxj6RQDv"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"ejOW2TLyTGdb","outputId":"0b43977a-8ed4-4929-e34d-bf4c5bd67942","executionInfo":{"status":"ok","timestamp":1725613195403,"user_tz":-330,"elapsed":47523,"user":{"displayName":"Jatin Agarwal","userId":"14406162079289589360"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["PDF Query Assistant with Llumo Compression\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-ab726d9f-79d2-4c5a-8af2-5efe1d31f2be\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-ab726d9f-79d2-4c5a-8af2-5efe1d31f2be\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Nikhil_Kamath.pdf to Nikhil_Kamath (1).pdf\n","Processing PDF...\n","Ask any question related to the content of the PDF: How did Nikhil Kamath started career?\n","Llumo Compression achieved: 55.15%\n","Initial tokens: 709\n","Final tokens: 318\n","Answer:\n","Nikhil Kamath started his career working at a call center while also trading in equity markets. He eventually became a stock trader and broker, founding Zerodha, a brokerage firm, with his brother Nitin.\n"]}]}]}